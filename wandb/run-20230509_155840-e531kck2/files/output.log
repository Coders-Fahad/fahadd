INFO:root:Starting...
INFO:root:Loading MR data...
________________________________________________________________________________
[Memory] Calling data.load_data...
load_data('MR')
________________________________________________________load_data - 4.0s, 0.1min
bert-base-uncased
Downloading (…)okenizer_config.json: 100%|██████████████████████████████████████████████████████████████████| 28.0/28.0 [00:00<00:00, 441B/s]
C:\Users\cyco\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\cyco\.cache\huggingface\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
Downloading (…)lve/main/config.json: 100%|██████████████████████████████████████████████████████████████████████████| 570/570 [00:00<?, ?B/s]
Downloading (…)solve/main/vocab.txt: 100%|█████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 553kB/s]
Downloading (…)/main/tokenizer.json: 100%|█████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 722kB/s]



















Downloading pytorch_model.bin: 100%|██████████████████████████████████████████████████████████████████████| 440M/440M [00:38<00:00, 11.5MB/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\cyco\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(





























































